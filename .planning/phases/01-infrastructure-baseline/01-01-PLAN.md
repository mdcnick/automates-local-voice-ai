---
phase: 01-infrastructure-baseline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docker-compose.yml
  - livekit_agent/src/agent.py
autonomous: true
requirements:
  - INFR-01
  - INFR-03
  - INFR-04

user_setup:
  - service: openrouter
    why: "LLM inference via cloud API"
    env_vars:
      - name: OPENROUTER_API_KEY
        source: "https://openrouter.ai/keys — create an API key"
    dashboard_config: []

must_haves:
  truths:
    - "Agent connects to OpenRouter for LLM inference instead of local llama_cpp"
    - "LLM model is configurable via LLM_MODEL environment variable without code changes"
    - "docker compose up starts cleanly without llama_cpp running"
    - "Local llama_cpp is still available via docker compose --profile local-llm up"
  artifacts:
    - path: "docker-compose.yml"
      provides: "llama_cpp behind profile, depends_on removed from livekit_agent"
      contains: "profiles: [\"local-llm\"]"
    - path: "livekit_agent/src/agent.py"
      provides: "OpenRouter LLM with FallbackAdapter to local llama_cpp"
      contains: "with_openrouter"
  key_links:
    - from: "livekit_agent/src/agent.py"
      to: "OpenRouter API"
      via: "openai.LLM.with_openrouter()"
      pattern: "LLM\\.with_openrouter"
    - from: "livekit_agent/src/agent.py"
      to: "LLM_MODEL env var"
      via: "os.getenv"
      pattern: "os\\.getenv.*LLM_MODEL"
    - from: "livekit_agent/src/agent.py"
      to: "local llama_cpp"
      via: "FallbackAdapter wrapping local LLM as last resort"
      pattern: "FallbackAdapter"
---

<objective>
Migrate LLM inference from local llama.cpp to OpenRouter and clean up Docker Compose.

Purpose: Establish cloud-connected agent with 70B+ model access, removing the local LLM as the default while preserving it as an emergency fallback behind a Docker Compose profile.

Output: Working agent using OpenRouter for LLM, docker-compose.yml with llama_cpp profiled, FallbackAdapter wiring to local LLM as last resort.
</objective>

<execution_context>
@/home/nc773/.claude/get-shit-done/workflows/execute-plan.md
@/home/nc773/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-infrastructure-baseline/01-CONTEXT.md
@.planning/phases/01-infrastructure-baseline/01-RESEARCH.md
@livekit_agent/src/agent.py
@docker-compose.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Move llama_cpp to Docker Compose profile and update depends_on</name>
  <files>docker-compose.yml</files>
  <action>
In docker-compose.yml:

1. Add `profiles: ["local-llm"]` to the `llama_cpp` service (first line under `llama_cpp:`).

2. Remove the `llama_cpp` entry from `livekit_agent.depends_on`. The remaining depends_on entries are: livekit (service_started), kokoro (service_started), nemotron (service_healthy).

3. Add `OPENROUTER_API_KEY` to the livekit_agent environment section:
   ```yaml
   - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
   ```

4. Add `LLM_MODEL` to the livekit_agent environment section:
   ```yaml
   - LLM_MODEL=${LLM_MODEL:-meta-llama/llama-3.3-70b-instruct:free}
   ```

Do NOT remove or rename any existing llama_cpp environment variables — they are still used by the profiled service.
  </action>
  <verify>
    <automated>cd /home/nc773/Documents/automates-local-voice-ai && docker compose config --services 2>&1 | grep -v llama_cpp && echo "PASS: llama_cpp not in default services" || echo "FAIL"</automated>
    <manual>Verify llama_cpp appears under profiles in docker compose config output</manual>
  </verify>
  <done>docker compose up no longer starts llama_cpp by default; llama_cpp is available with --profile local-llm; OPENROUTER_API_KEY and LLM_MODEL are passed to livekit_agent</done>
</task>

<task type="auto">
  <name>Task 2: Wire OpenRouter LLM with FallbackAdapter to local llama_cpp</name>
  <files>livekit_agent/src/agent.py</files>
  <action>
Replace the current LLM configuration in agent.py with OpenRouter via `with_openrouter()` and a FallbackAdapter to local llama_cpp.

1. Add import at top: `from livekit.agents.llm import FallbackAdapter`

2. Create a `build_llm()` helper function (outside `my_agent`) that:
   - Reads `LLM_MODEL` env var (default: `meta-llama/llama-3.3-70b-instruct:free`)
   - Splits on comma to get model chain: `[m.strip() for m in raw.split(",")]`
   - Creates OpenRouter LLM: `openai.LLM.with_openrouter(model=chain[0], fallback_models=chain[1:])`
   - Creates local llama_cpp LLM: `openai.LLM(base_url=os.getenv("LOCAL_LLM_BASE_URL", "http://llama_cpp:11434/v1"), model=os.getenv("LOCAL_LLM_MODEL", "qwen3-4b"), api_key="no-key-needed")`
   - Returns `FallbackAdapter([openrouter_llm, local_llm])`

3. In `my_agent()`, replace the existing llama_model/llama_base_url/llama_api_key block and the `llm=openai.LLM(...)` in `AgentSession` with `llm=build_llm()`.

4. Remove the old `llama_model`, `llama_base_url`, `llama_api_key` variable assignments (lines 62-64). Keep STT and TTS configuration unchanged.

5. Update the logger.info call to log the LLM_MODEL env var value instead of llama details.

Keep all STT, TTS, VAD, and turn detection configuration exactly as-is. Do not change the Assistant class or its instructions.
  </action>
  <verify>
    <automated>cd /home/nc773/Documents/automates-local-voice-ai/livekit_agent && uv run ruff check src/agent.py && uv run ruff format --check src/agent.py && echo "PASS: lint and format clean"</automated>
    <manual>Review agent.py to confirm with_openrouter is used and FallbackAdapter wraps both LLMs</manual>
  </verify>
  <done>agent.py uses openai.LLM.with_openrouter() for primary LLM with FallbackAdapter to local llama_cpp; LLM_MODEL env var controls model selection with comma-separated fallback chain; old llama_model/llama_base_url/llama_api_key variables removed</done>
</task>

</tasks>

<verification>
1. `docker compose config --services` does NOT list llama_cpp
2. `docker compose --profile local-llm config --services` DOES list llama_cpp
3. `ruff check` and `ruff format --check` pass on agent.py
4. agent.py contains `with_openrouter`, `FallbackAdapter`, `LLM_MODEL`
5. agent.py does NOT contain `llama_base_url` or `llama_api_key` variable assignments
</verification>

<success_criteria>
- OpenRouter is the primary LLM provider in agent.py
- Model configurable via LLM_MODEL env var (comma-separated chain)
- FallbackAdapter wraps OpenRouter + local llama_cpp
- docker-compose.yml: llama_cpp behind profile, depends_on cleaned up
- Code passes ruff lint and format checks
</success_criteria>

<output>
After completion, create `.planning/phases/01-infrastructure-baseline/01-01-SUMMARY.md`
</output>

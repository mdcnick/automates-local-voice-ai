services:
  kokoro:
    build:
      context: ./inference/kokoro
    ports:
      - "8880:8880"
    networks:
      - agent_network

  nemotron:
    profiles: ["nemotron"]
    build:
      context: ./inference/nemotron
      args:
        NEMOTRON_BASE: ${NEMOTRON_BASE:-python:3.10-slim}
        TORCH_INDEX_URL: ${NEMOTRON_TORCH_INDEX_URL:-https://download.pytorch.org/whl/cpu}
    volumes:
      - nemotron-cache:/root/.cache/huggingface
    environment:
      - NEMOTRON_MODEL_NAME=${NEMOTRON_MODEL_NAME:-nvidia/nemotron-speech-streaming-en-0.6b}
      - NEMOTRON_MODEL_ID=${NEMOTRON_MODEL_ID:-nemotron-speech-streaming}
      - PYTORCH_ENABLE_MPS_FALLBACK=1
    ports:
      - "11435:8000"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8000/health | grep -q '\"model_loaded\":true'"]
      interval: 10s
      timeout: 5s
      retries: 120
      start_period: 20s
    networks:
      - agent_network

  whisper:
    profiles: ["whisper"]
    build:
      context: ./inference/whisper
    volumes:
      - whisper-data:/data
    environment:
      - VOXBOX_HF_REPO_ID=${VOXBOX_HF_REPO_ID:-Systran/faster-whisper-small}
      - VOXBOX_DEVICE=${VOXBOX_DEVICE:-cpu}
      - DATA_DIR=/data
    ports:
      - "11437:80"
    networks:
      - agent_network

  llama_cpp:
    profiles: ["local-llm"]
    image: ghcr.io/ggml-org/llama.cpp:server
    command:
      - --host
      - 0.0.0.0
      - --port
      - "11434"
      - --hf-repo
      - "${LLAMA_HF_REPO:-unsloth/Qwen3-4B-Instruct-2507-GGUF}"
      - --alias
      - "${LLAMA_MODEL_ALIAS:-qwen3-4b}"
      - --ctx-size
      - "${LLAMA_CTX_SIZE:-16384}"
    ports:
      - "${LLAMA_HOST_PORT:-11436}:11434"
    volumes:
      - ./inference/llama/models:/models
    environment:
      - XDG_CACHE_HOME=/models
      - HF_HOME=/models
    networks:
      - agent_network
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:11434/v1/models > /dev/null"]
      interval: 10s
      timeout: 5s
      retries: 30

  livekit_agent:
    build:
      context: ./livekit_agent
    env_file:
      - .env
      - path: .env.local
        required: false
    environment:
      - LIVEKIT_URL=${LIVEKIT_URL}
      - LIVEKIT_HOST=${LIVEKIT_URL}
      - LIVEKIT_API_KEY=${LIVEKIT_API_KEY}
      - LIVEKIT_API_SECRET=${LIVEKIT_API_SECRET}
      - LIVEKIT_AGENT_PORT=${LIVEKIT_AGENT_PORT:-7880}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-no-key-needed}
      - GROQ_API_KEY=${GROQ_API_KEY:-no-key-needed}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY:-}
      - LLM_MODEL=${LLM_MODEL:-meta-llama/llama-3.3-70b-instruct:free}
      - DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY:-}
    depends_on:
      kokoro:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8081/ > /dev/null"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s
    networks:
      - agent_network

  frontend:
    build:
      context: ./frontend
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_LIVEKIT_URL=${NEXT_PUBLIC_LIVEKIT_URL}
      - LIVEKIT_URL=${LIVEKIT_URL}
      - LIVEKIT_API_KEY=${LIVEKIT_API_KEY}
      - LIVEKIT_API_SECRET=${LIVEKIT_API_SECRET}
    networks:
      - agent_network

volumes:
  nemotron-cache:
  whisper-data:

networks:
  agent_network:
    driver: bridge
